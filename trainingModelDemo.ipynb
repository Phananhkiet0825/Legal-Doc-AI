{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad046d6-dfd7-49fd-bb20-2fb36a348905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:utf-8-sig encoding failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte. Trying utf-16...\n",
      "Encoding Batches: 100%|███████████████████████████████████████████████████████████████| 290/290 [00:34<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top Related Articles:\n",
      "\n",
      "📘 Điều 179 (Similarity: 0.68)\n",
      "📝 Hợp đồng gia công phải được lập thành văn bản hoặc bằng hình thức khác có giá trị pháp lý tương đương.\n",
      "\n",
      "📘 Điều 251 (Similarity: 0.67)\n",
      "📝 Hợp đồng dịch vụ quá cảnh phải được lập thành văn bản hoặc bằng hình thức khác có giá trị pháp lý tương đương.\n",
      "\n",
      "📘 Điều 159 (Similarity: 0.67)\n",
      "📝 Hợp đồng uỷ thác mua bán hàng hoá phải được lập thành văn bản hoặc bằng hình thức khác có giá trị pháp lý tương đương.\n",
      "\n",
      "📘 Điều 168 (Similarity: 0.66)\n",
      "📝 Hợp đồng đại lý phải được lập thành văn bản hoặc bằng hình thức khác có giá trị pháp lý tương đương.\n",
      "\n",
      "📘 Điều 142 (Similarity: 0.65)\n",
      "📝 Hợp đồng đại diện cho thương nhân phải được lập thành văn bản hoặc bằng hình thức khác có giá trị pháp lý tương đương.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Try importing faiss for efficient similarity search.\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    faiss = None\n",
    "    logging.info(\"Faiss is not installed; falling back to cosine similarity.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LegalDocumentFinder:\n",
    "    def __init__(self, model_name: str = \"vinai/phobert-base\", device: str = None):\n",
    "        # Set device to CUDA if available, unless explicitly specified.\n",
    "        self.device = torch.device(device) if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(\"Using device: %s\", self.device)\n",
    "        \n",
    "        # Load tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.embeddings = None  # Numpy array containing embeddings.\n",
    "        self.df = None          # DataFrame with the legal documents.\n",
    "        self.index = None       # FAISS index for efficient similarity search (optional).\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load the CSV file with legal texts, handling common encoding issues.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='\"')\n",
    "            logger.info(\"Loaded data using utf-8-sig encoding.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"utf-8-sig encoding failed: %s. Trying utf-16...\", e)\n",
    "            df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='\"')\n",
    "        # Clean DataFrame: remove rows with missing text or duplicates.\n",
    "        df = df.dropna(subset=[\"truncated_text\", \"dieu\"]).drop_duplicates(subset=[\"truncated_text\"])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        logger.info(\"Data cleaned: %d rows.\", len(df))\n",
    "        return df\n",
    "\n",
    "    def batch_encode_texts(self, texts: list, batch_size: int = 32, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode texts in batches using GPU if available.\"\"\"\n",
    "        embeddings = []\n",
    "        logger.info(\"Starting batch encoding for %d texts...\", len(texts))\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch_texts = texts[i: i + batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                                      max_length=max_length, padding=\"max_length\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extract the [CLS] token embedding from the last hidden state.\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        logger.info(\"Completed encoding. Embeddings shape: %s\", embeddings.shape)\n",
    "        return embeddings\n",
    "\n",
    "    def cache_embeddings(self, cache_path: str):\n",
    "        \"\"\"Save computed embeddings and DataFrame to disk.\"\"\"\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump({\"df\": self.df, \"embeddings\": self.embeddings}, f)\n",
    "        logger.info(\"Embeddings cached to %s.\", cache_path)\n",
    "\n",
    "    def load_cached_embeddings(self, cache_path: str) -> bool:\n",
    "        \"\"\"Load cached embeddings and DataFrame from disk if available.\"\"\"\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "                self.df = cache[\"df\"]\n",
    "                self.embeddings = cache[\"embeddings\"]\n",
    "            logger.info(\"Loaded cached embeddings from %s.\", cache_path)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"Build an index for efficient similarity search using FAISS, if available.\"\"\"\n",
    "        if faiss is not None and self.embeddings is not None:\n",
    "            d = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(d)\n",
    "            # Normalize the embeddings for cosine similarity approximation.\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "            self.index.add(self.embeddings.astype(np.float32))\n",
    "            logger.info(\"FAISS index built with %d vectors.\", self.index.ntotal)\n",
    "        else:\n",
    "            logger.info(\"FAISS not available or embeddings not computed; skipping index build.\")\n",
    "\n",
    "    def encode_text(self, text: str, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode a single input text into its embedding.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                                max_length=max_length, padding=\"max_length\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "    def find_similar_articles(self, input_text: str, top_k: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Retrieve the top_k similar legal documents for a given input text.\"\"\"\n",
    "        input_embedding = self.encode_text(input_text).reshape(1, -1)\n",
    "        \n",
    "        # Normalize the input embedding.\n",
    "        input_norm = input_embedding / np.linalg.norm(input_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        if self.index is not None:\n",
    "            # Use FAISS index for similarity search.\n",
    "            input_embedding_norm = input_norm.astype(np.float32)\n",
    "            distances, indices = self.index.search(input_embedding_norm, top_k)\n",
    "            # Approximate cosine similarity from Euclidean distances.\n",
    "            similarities = 1 - distances.flatten() / 2  \n",
    "            top_indices = indices.flatten()\n",
    "            logger.info(\"Similar articles found using FAISS.\")\n",
    "        else:\n",
    "            # Fall back to cosine similarity calculation.\n",
    "            matrix = np.stack(self.df[\"embedding\"].values)\n",
    "            similarities = cosine_similarity(input_embedding, matrix).flatten()\n",
    "            top_indices = similarities.argsort()[::-1][:top_k]\n",
    "            similarities = similarities[top_indices]\n",
    "            logger.info(\"Similar articles found using cosine similarity.\")\n",
    "        \n",
    "        results = self.df.iloc[top_indices][[\"dieu\", \"truncated_text\"]].copy()\n",
    "        results[\"similarity\"] = similarities\n",
    "        return results\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Automated System for Identifying Vietnamese Legal Documents\")\n",
    "    parser.add_argument(\"--data_file\", type=str, default=\"sent_truncated_vbpl_legal_only.csv\",\n",
    "                        help=\"CSV file path containing legal texts.\")\n",
    "    parser.add_argument(\"--cache_file\", type=str, default=\"embeddings_cache.pkl\",\n",
    "                        help=\"Path to save/load embeddings cache.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for text encoding.\")\n",
    "    parser.add_argument(\"--use_faiss\", action=\"store_true\", help=\"Build FAISS index for similarity search (if installed).\")\n",
    "    # Use parse_known_args to handle extra parameters injected by the Jupyter kernel.\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        logger.debug(\"Ignoring unknown arguments: %s\", unknown)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Initialize the legal document finder.\n",
    "    finder = LegalDocumentFinder()\n",
    "\n",
    "    # Load and preprocess data.\n",
    "    finder.df = finder.load_data(args.data_file)\n",
    "\n",
    "    # Check for cached embeddings.\n",
    "    if not finder.load_cached_embeddings(args.cache_file):\n",
    "        all_texts = finder.df[\"truncated_text\"].tolist()\n",
    "        finder.embeddings = finder.batch_encode_texts(all_texts, batch_size=args.batch_size)\n",
    "        # Store embeddings in the DataFrame for later use.\n",
    "        finder.df[\"embedding\"] = list(finder.embeddings)\n",
    "        finder.cache_embeddings(args.cache_file)\n",
    "    \n",
    "    # Build FAISS index if the flag is set.\n",
    "    if args.use_faiss:\n",
    "        finder.build_index()\n",
    "    \n",
    "    # Demo search.\n",
    "    demo_text = \"Biên bản họp Hội đồng thành viên phải được giữ nguyên và không được phép thay đổi\"\n",
    "    results = finder.find_similar_articles(demo_text)\n",
    "    \n",
    "    # Output the results.\n",
    "    print(\"\\n🔍 Top Related Articles:\")\n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"\\n📘 {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"📝 {row['truncated_text']}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246eb7c6-ff25-43fd-a64a-f1e047c18e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:utf-8-sig encoding failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte. Trying utf-16...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top Related Articles:\n",
      "\n",
      "📘 Điều 392 (Similarity: 0.56)\n",
      "📝 Sửa đổi đề nghị do bên được đề nghị đề xuất Khi bên được đề nghị đã chấp nhận giao kết hợp đồng nhưng có nêu điều kiện hoặc sửa đổi đề nghị thì coi như người này đã đưa ra đề nghị mới.\n",
      "\n",
      "📘 Điều 12 (Similarity: 0.56)\n",
      "📝 Trừ trường hợp có thoả thuận khác, các bên được coi là mặc nhiên áp dụng thói quen trong hoạt động thương mại đã được thiết lập giữa các bên đó mà các bên đã biết hoặc phải biết nhưng không được trái với quy định của pháp luật.\n",
      "\n",
      "📘 Điều 116 (Similarity: 0.55)\n",
      "📝 Người tham gia thủ tục phá sản là người nước ngoài Người tham gia thủ tục phá sản là người nước ngoài phải thực hiện theo quy định của pháp luật về phá sản của Việt Nam.\n",
      "\n",
      "📘 Điều 644 (Similarity: 0.54)\n",
      "📝 Việc thừa kế của những người có quyền thừa kế di sản của nhau mà chết trong cùng một thời điểm Trong trường hợp những người có quyền thừa kế di sản của nhau đều chết trong cùng một thời điểm hoặc được coi là chết trong cùng một thời điểm do không thể xác định được người nào chết trước, thì họ không được thừa kế di sản của nhau và di sản của mỗi người do người thừa kế của người đó hưởng.\n",
      "\n",
      "📘 Điều 468 (Similarity: 0.54)\n",
      "📝 Trách nhiệm do cố ý tặng cho tài sản không thuộc sở hữu của mình Trong trường hợp bên tặng cho cố ý tặng cho tài sản không thuộc sở hữu của mình mà bên được tặng cho không biết hoặc không thể biết về việc đó thì bên tặng cho phải thanh toán chi phí để làm tăng giá trị của tài sản cho bên được tặng cho khi chủ sở hữu lấy lại tài sản.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Initialize the legal document finder.\n",
    "    finder = LegalDocumentFinder()\n",
    "\n",
    "    # Load and preprocess data.\n",
    "    finder.df = finder.load_data(args.data_file)\n",
    "\n",
    "    # Check for cached embeddings.\n",
    "    if not finder.load_cached_embeddings(args.cache_file):\n",
    "        all_texts = finder.df[\"truncated_text\"].tolist()\n",
    "        finder.embeddings = finder.batch_encode_texts(all_texts, batch_size=args.batch_size)\n",
    "        # Store embeddings in the DataFrame for later use.\n",
    "        finder.df[\"embedding\"] = list(finder.embeddings)\n",
    "        finder.cache_embeddings(args.cache_file)\n",
    "    \n",
    "    # Build FAISS index if the flag is set.\n",
    "    if args.use_faiss:\n",
    "        finder.build_index()\n",
    "    \n",
    "    # Demo search.\n",
    "    demo_text = \"không được phép thay đổi\"\n",
    "    results = finder.find_similar_articles(demo_text)\n",
    "    \n",
    "    # Output the results.\n",
    "    print(\"\\n🔍 Top Related Articles:\")\n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"\\n📘 {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"📝 {row['truncated_text']}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29ef08-ee6e-4e09-805e-43380181527d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
