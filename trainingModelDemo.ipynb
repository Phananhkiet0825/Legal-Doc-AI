{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad046d6-dfd7-49fd-bb20-2fb36a348905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:utf-8-sig encoding failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte. Trying utf-16...\n",
      "Encoding Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:34<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Top Related Articles:\n",
      "\n",
      "ğŸ“˜ Äiá»u 179 (Similarity: 0.68)\n",
      "ğŸ“ Há»£p Ä‘á»“ng gia cÃ´ng pháº£i Ä‘Æ°á»£c láº­p thÃ nh vÄƒn báº£n hoáº·c báº±ng hÃ¬nh thá»©c khÃ¡c cÃ³ giÃ¡ trá»‹ phÃ¡p lÃ½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng.\n",
      "\n",
      "ğŸ“˜ Äiá»u 251 (Similarity: 0.67)\n",
      "ğŸ“ Há»£p Ä‘á»“ng dá»‹ch vá»¥ quÃ¡ cáº£nh pháº£i Ä‘Æ°á»£c láº­p thÃ nh vÄƒn báº£n hoáº·c báº±ng hÃ¬nh thá»©c khÃ¡c cÃ³ giÃ¡ trá»‹ phÃ¡p lÃ½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng.\n",
      "\n",
      "ğŸ“˜ Äiá»u 159 (Similarity: 0.67)\n",
      "ğŸ“ Há»£p Ä‘á»“ng uá»· thÃ¡c mua bÃ¡n hÃ ng hoÃ¡ pháº£i Ä‘Æ°á»£c láº­p thÃ nh vÄƒn báº£n hoáº·c báº±ng hÃ¬nh thá»©c khÃ¡c cÃ³ giÃ¡ trá»‹ phÃ¡p lÃ½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng.\n",
      "\n",
      "ğŸ“˜ Äiá»u 168 (Similarity: 0.66)\n",
      "ğŸ“ Há»£p Ä‘á»“ng Ä‘áº¡i lÃ½ pháº£i Ä‘Æ°á»£c láº­p thÃ nh vÄƒn báº£n hoáº·c báº±ng hÃ¬nh thá»©c khÃ¡c cÃ³ giÃ¡ trá»‹ phÃ¡p lÃ½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng.\n",
      "\n",
      "ğŸ“˜ Äiá»u 142 (Similarity: 0.65)\n",
      "ğŸ“ Há»£p Ä‘á»“ng Ä‘áº¡i diá»‡n cho thÆ°Æ¡ng nhÃ¢n pháº£i Ä‘Æ°á»£c láº­p thÃ nh vÄƒn báº£n hoáº·c báº±ng hÃ¬nh thá»©c khÃ¡c cÃ³ giÃ¡ trá»‹ phÃ¡p lÃ½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Try importing faiss for efficient similarity search.\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    faiss = None\n",
    "    logging.info(\"Faiss is not installed; falling back to cosine similarity.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LegalDocumentFinder:\n",
    "    def __init__(self, model_name: str = \"vinai/phobert-base\", device: str = None):\n",
    "        # Set device to CUDA if available, unless explicitly specified.\n",
    "        self.device = torch.device(device) if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(\"Using device: %s\", self.device)\n",
    "        \n",
    "        # Load tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.embeddings = None  # Numpy array containing embeddings.\n",
    "        self.df = None          # DataFrame with the legal documents.\n",
    "        self.index = None       # FAISS index for efficient similarity search (optional).\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load the CSV file with legal texts, handling common encoding issues.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='\"')\n",
    "            logger.info(\"Loaded data using utf-8-sig encoding.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"utf-8-sig encoding failed: %s. Trying utf-16...\", e)\n",
    "            df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='\"')\n",
    "        # Clean DataFrame: remove rows with missing text or duplicates.\n",
    "        df = df.dropna(subset=[\"truncated_text\", \"dieu\"]).drop_duplicates(subset=[\"truncated_text\"])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        logger.info(\"Data cleaned: %d rows.\", len(df))\n",
    "        return df\n",
    "\n",
    "    def batch_encode_texts(self, texts: list, batch_size: int = 32, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode texts in batches using GPU if available.\"\"\"\n",
    "        embeddings = []\n",
    "        logger.info(\"Starting batch encoding for %d texts...\", len(texts))\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch_texts = texts[i: i + batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                                      max_length=max_length, padding=\"max_length\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extract the [CLS] token embedding from the last hidden state.\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        logger.info(\"Completed encoding. Embeddings shape: %s\", embeddings.shape)\n",
    "        return embeddings\n",
    "\n",
    "    def cache_embeddings(self, cache_path: str):\n",
    "        \"\"\"Save computed embeddings and DataFrame to disk.\"\"\"\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump({\"df\": self.df, \"embeddings\": self.embeddings}, f)\n",
    "        logger.info(\"Embeddings cached to %s.\", cache_path)\n",
    "\n",
    "    def load_cached_embeddings(self, cache_path: str) -> bool:\n",
    "        \"\"\"Load cached embeddings and DataFrame from disk if available.\"\"\"\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "                self.df = cache[\"df\"]\n",
    "                self.embeddings = cache[\"embeddings\"]\n",
    "            logger.info(\"Loaded cached embeddings from %s.\", cache_path)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"Build an index for efficient similarity search using FAISS, if available.\"\"\"\n",
    "        if faiss is not None and self.embeddings is not None:\n",
    "            d = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(d)\n",
    "            # Normalize the embeddings for cosine similarity approximation.\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "            self.index.add(self.embeddings.astype(np.float32))\n",
    "            logger.info(\"FAISS index built with %d vectors.\", self.index.ntotal)\n",
    "        else:\n",
    "            logger.info(\"FAISS not available or embeddings not computed; skipping index build.\")\n",
    "\n",
    "    def encode_text(self, text: str, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode a single input text into its embedding.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                                max_length=max_length, padding=\"max_length\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "    def find_similar_articles(self, input_text: str, top_k: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Retrieve the top_k similar legal documents for a given input text.\"\"\"\n",
    "        input_embedding = self.encode_text(input_text).reshape(1, -1)\n",
    "        \n",
    "        # Normalize the input embedding.\n",
    "        input_norm = input_embedding / np.linalg.norm(input_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        if self.index is not None:\n",
    "            # Use FAISS index for similarity search.\n",
    "            input_embedding_norm = input_norm.astype(np.float32)\n",
    "            distances, indices = self.index.search(input_embedding_norm, top_k)\n",
    "            # Approximate cosine similarity from Euclidean distances.\n",
    "            similarities = 1 - distances.flatten() / 2  \n",
    "            top_indices = indices.flatten()\n",
    "            logger.info(\"Similar articles found using FAISS.\")\n",
    "        else:\n",
    "            # Fall back to cosine similarity calculation.\n",
    "            matrix = np.stack(self.df[\"embedding\"].values)\n",
    "            similarities = cosine_similarity(input_embedding, matrix).flatten()\n",
    "            top_indices = similarities.argsort()[::-1][:top_k]\n",
    "            similarities = similarities[top_indices]\n",
    "            logger.info(\"Similar articles found using cosine similarity.\")\n",
    "        \n",
    "        results = self.df.iloc[top_indices][[\"dieu\", \"truncated_text\"]].copy()\n",
    "        results[\"similarity\"] = similarities\n",
    "        return results\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Automated System for Identifying Vietnamese Legal Documents\")\n",
    "    parser.add_argument(\"--data_file\", type=str, default=\"sent_truncated_vbpl_legal_only.csv\",\n",
    "                        help=\"CSV file path containing legal texts.\")\n",
    "    parser.add_argument(\"--cache_file\", type=str, default=\"embeddings_cache.pkl\",\n",
    "                        help=\"Path to save/load embeddings cache.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for text encoding.\")\n",
    "    parser.add_argument(\"--use_faiss\", action=\"store_true\", help=\"Build FAISS index for similarity search (if installed).\")\n",
    "    # Use parse_known_args to handle extra parameters injected by the Jupyter kernel.\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        logger.debug(\"Ignoring unknown arguments: %s\", unknown)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Initialize the legal document finder.\n",
    "    finder = LegalDocumentFinder()\n",
    "\n",
    "    # Load and preprocess data.\n",
    "    finder.df = finder.load_data(args.data_file)\n",
    "\n",
    "    # Check for cached embeddings.\n",
    "    if not finder.load_cached_embeddings(args.cache_file):\n",
    "        all_texts = finder.df[\"truncated_text\"].tolist()\n",
    "        finder.embeddings = finder.batch_encode_texts(all_texts, batch_size=args.batch_size)\n",
    "        # Store embeddings in the DataFrame for later use.\n",
    "        finder.df[\"embedding\"] = list(finder.embeddings)\n",
    "        finder.cache_embeddings(args.cache_file)\n",
    "    \n",
    "    # Build FAISS index if the flag is set.\n",
    "    if args.use_faiss:\n",
    "        finder.build_index()\n",
    "    \n",
    "    # Demo search.\n",
    "    demo_text = \"BiÃªn báº£n há»p Há»™i Ä‘á»“ng thÃ nh viÃªn pháº£i Ä‘Æ°á»£c giá»¯ nguyÃªn vÃ  khÃ´ng Ä‘Æ°á»£c phÃ©p thay Ä‘á»•i\"\n",
    "    results = finder.find_similar_articles(demo_text)\n",
    "    \n",
    "    # Output the results.\n",
    "    print(\"\\nğŸ” Top Related Articles:\")\n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"\\nğŸ“˜ {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"ğŸ“ {row['truncated_text']}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246eb7c6-ff25-43fd-a64a-f1e047c18e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:utf-8-sig encoding failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte. Trying utf-16...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Top Related Articles:\n",
      "\n",
      "ğŸ“˜ Äiá»u 392 (Similarity: 0.56)\n",
      "ğŸ“ Sá»­a Ä‘á»•i Ä‘á» nghá»‹ do bÃªn Ä‘Æ°á»£c Ä‘á» nghá»‹ Ä‘á» xuáº¥t Khi bÃªn Ä‘Æ°á»£c Ä‘á» nghá»‹ Ä‘Ã£ cháº¥p nháº­n giao káº¿t há»£p Ä‘á»“ng nhÆ°ng cÃ³ nÃªu Ä‘iá»u kiá»‡n hoáº·c sá»­a Ä‘á»•i Ä‘á» nghá»‹ thÃ¬ coi nhÆ° ngÆ°á»i nÃ y Ä‘Ã£ Ä‘Æ°a ra Ä‘á» nghá»‹ má»›i.\n",
      "\n",
      "ğŸ“˜ Äiá»u 12 (Similarity: 0.56)\n",
      "ğŸ“ Trá»« trÆ°á»ng há»£p cÃ³ thoáº£ thuáº­n khÃ¡c, cÃ¡c bÃªn Ä‘Æ°á»£c coi lÃ  máº·c nhiÃªn Ã¡p dá»¥ng thÃ³i quen trong hoáº¡t Ä‘á»™ng thÆ°Æ¡ng máº¡i Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p giá»¯a cÃ¡c bÃªn Ä‘Ã³ mÃ  cÃ¡c bÃªn Ä‘Ã£ biáº¿t hoáº·c pháº£i biáº¿t nhÆ°ng khÃ´ng Ä‘Æ°á»£c trÃ¡i vá»›i quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t.\n",
      "\n",
      "ğŸ“˜ Äiá»u 116 (Similarity: 0.55)\n",
      "ğŸ“ NgÆ°á»i tham gia thá»§ tá»¥c phÃ¡ sáº£n lÃ  ngÆ°á»i nÆ°á»›c ngoÃ i NgÆ°á»i tham gia thá»§ tá»¥c phÃ¡ sáº£n lÃ  ngÆ°á»i nÆ°á»›c ngoÃ i pháº£i thá»±c hiá»‡n theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t vá» phÃ¡ sáº£n cá»§a Viá»‡t Nam.\n",
      "\n",
      "ğŸ“˜ Äiá»u 644 (Similarity: 0.54)\n",
      "ğŸ“ Viá»‡c thá»«a káº¿ cá»§a nhá»¯ng ngÆ°á»i cÃ³ quyá»n thá»«a káº¿ di sáº£n cá»§a nhau mÃ  cháº¿t trong cÃ¹ng má»™t thá»i Ä‘iá»ƒm Trong trÆ°á»ng há»£p nhá»¯ng ngÆ°á»i cÃ³ quyá»n thá»«a káº¿ di sáº£n cá»§a nhau Ä‘á»u cháº¿t trong cÃ¹ng má»™t thá»i Ä‘iá»ƒm hoáº·c Ä‘Æ°á»£c coi lÃ  cháº¿t trong cÃ¹ng má»™t thá»i Ä‘iá»ƒm do khÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c ngÆ°á»i nÃ o cháº¿t trÆ°á»›c, thÃ¬ há» khÃ´ng Ä‘Æ°á»£c thá»«a káº¿ di sáº£n cá»§a nhau vÃ  di sáº£n cá»§a má»—i ngÆ°á»i do ngÆ°á»i thá»«a káº¿ cá»§a ngÆ°á»i Ä‘Ã³ hÆ°á»Ÿng.\n",
      "\n",
      "ğŸ“˜ Äiá»u 468 (Similarity: 0.54)\n",
      "ğŸ“ TrÃ¡ch nhiá»‡m do cá»‘ Ã½ táº·ng cho tÃ i sáº£n khÃ´ng thuá»™c sá»Ÿ há»¯u cá»§a mÃ¬nh Trong trÆ°á»ng há»£p bÃªn táº·ng cho cá»‘ Ã½ táº·ng cho tÃ i sáº£n khÃ´ng thuá»™c sá»Ÿ há»¯u cá»§a mÃ¬nh mÃ  bÃªn Ä‘Æ°á»£c táº·ng cho khÃ´ng biáº¿t hoáº·c khÃ´ng thá»ƒ biáº¿t vá» viá»‡c Ä‘Ã³ thÃ¬ bÃªn táº·ng cho pháº£i thanh toÃ¡n chi phÃ­ Ä‘á»ƒ lÃ m tÄƒng giÃ¡ trá»‹ cá»§a tÃ i sáº£n cho bÃªn Ä‘Æ°á»£c táº·ng cho khi chá»§ sá»Ÿ há»¯u láº¥y láº¡i tÃ i sáº£n.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Initialize the legal document finder.\n",
    "    finder = LegalDocumentFinder()\n",
    "\n",
    "    # Load and preprocess data.\n",
    "    finder.df = finder.load_data(args.data_file)\n",
    "\n",
    "    # Check for cached embeddings.\n",
    "    if not finder.load_cached_embeddings(args.cache_file):\n",
    "        all_texts = finder.df[\"truncated_text\"].tolist()\n",
    "        finder.embeddings = finder.batch_encode_texts(all_texts, batch_size=args.batch_size)\n",
    "        # Store embeddings in the DataFrame for later use.\n",
    "        finder.df[\"embedding\"] = list(finder.embeddings)\n",
    "        finder.cache_embeddings(args.cache_file)\n",
    "    \n",
    "    # Build FAISS index if the flag is set.\n",
    "    if args.use_faiss:\n",
    "        finder.build_index()\n",
    "    \n",
    "    # Demo search.\n",
    "    demo_text = \"khÃ´ng Ä‘Æ°á»£c phÃ©p thay Ä‘á»•i\"\n",
    "    results = finder.find_similar_articles(demo_text)\n",
    "    \n",
    "    # Output the results.\n",
    "    print(\"\\nğŸ” Top Related Articles:\")\n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"\\nğŸ“˜ {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "        print(f\"ğŸ“ {row['truncated_text']}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29ef08-ee6e-4e09-805e-43380181527d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
