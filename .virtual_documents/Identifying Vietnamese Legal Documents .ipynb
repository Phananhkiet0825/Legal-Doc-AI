# Cell 1: Dependencies and Logging Configuration
import os
import logging
import pickle
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity

# Try importing faiss for efficient similarity search.
try:
    import faiss
except ImportError:
    faiss = None
    logging.info("Faiss is not installed; falling back to cosine similarity.")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



# Cell 2: LegalDocumentFinder Class Definition with Save/Load Methods
class LegalDocumentFinder:
    def __init__(self, model_name: str = "vinai/phobert-base", device: str = None):
        # Set device to CUDA if available, otherwise CPU.
        self.device = torch.device(device) if device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # Load the tokenizer and model.
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        self.embeddings = None  # Will hold computed embeddings as a NumPy array.
        self.df = None          # DataFrame holding the legal documents.
        self.index = None       # FAISS index for fast similarity search (optional).

    def load_data(self, file_path: str) -> pd.DataFrame:
        """Load CSV data while handling common encoding issues."""
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='"')
            logger.info("Loaded data using utf-8-sig encoding.")
        except Exception as e:
            logger.warning(f"utf-8-sig encoding failed: {e}. Trying utf-16...")
            df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='"')
        # Remove rows with missing values and duplicates.
        df = df.dropna(subset=["truncated_text", "dieu"]).drop_duplicates(subset=["truncated_text"])
        df.reset_index(drop=True, inplace=True)
        logger.info(f"Data cleaned: {len(df)} rows.")
        self.df = df
        return df

    def batch_encode_texts(self, texts: list, batch_size: int = 32, max_length: int = 256) -> np.ndarray:
        """Encode texts in batches using GPU if available."""
        embeddings = []
        logger.info(f"Starting batch encoding for {len(texts)} texts...")
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding Batches"):
            batch_texts = texts[i: i + batch_size]
            inputs = self.tokenizer(batch_texts, return_tensors="pt", truncation=True,
                                      max_length=max_length, padding="max_length")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = self.model(**inputs)
            # Extract the [CLS] token embedding from the last hidden state.
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.append(batch_embeddings)
        embeddings = np.concatenate(embeddings, axis=0)
        logger.info(f"Completed encoding. Embeddings shape: {embeddings.shape}")
        self.embeddings = embeddings
        return embeddings

    def cache_embeddings(self, cache_path: str):
        """Cache embeddings and DataFrame to disk."""
        with open(cache_path, "wb") as f:
            pickle.dump({"df": self.df, "embeddings": self.embeddings}, f)
        logger.info(f"Embeddings cached to {cache_path}.")

    def load_cached_embeddings(self, cache_path: str) -> bool:
        """Load cached embeddings and DataFrame from disk if available."""
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                cache = pickle.load(f)
                self.df = cache["df"]
                self.embeddings = cache["embeddings"]
            logger.info(f"Loaded cached embeddings from {cache_path}.")
            return True
        return False

    def build_index(self):
        """Build a FAISS index for fast similarity search (if available)."""
        if faiss is not None and self.embeddings is not None:
            d = self.embeddings.shape[1]
            self.index = faiss.IndexFlatL2(d)
            # Normalize embeddings for cosine-similarity approximation.
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings.astype(np.float32))
            logger.info(f"FAISS index built with {self.index.ntotal} vectors.")
        else:
            logger.info("FAISS not available or embeddings not computed; skipping index build.")

    def encode_text(self, text: str, max_length: int = 256) -> np.ndarray:
        """Encode a single text into its vector representation."""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True,
                                max_length=max_length, padding="max_length")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

    def find_similar_articles(self, input_text: str, top_k: int = 5) -> pd.DataFrame:
        """Retrieve the top_k similar legal documents for the input text."""
        input_embedding = self.encode_text(input_text).reshape(1, -1)
        input_norm = input_embedding / np.linalg.norm(input_embedding, axis=1, keepdims=True)
        
        if self.index is not None:
            # Use FAISS index for efficient search.
            input_embedding_norm = input_norm.astype(np.float32)
            distances, indices = self.index.search(input_embedding_norm, top_k)
            # Approximate cosine similarity from L2 distances.
            similarities = 1 - distances.flatten() / 2  
            top_indices = indices.flatten()
            logger.info("Similar articles found using FAISS.")
        else:
            # Fall back to cosine similarity using scikit-learn.
            matrix = np.stack(self.df["embedding"].values)
            similarities = cosine_similarity(input_embedding, matrix).flatten()
            top_indices = similarities.argsort()[::-1][:top_k]
            similarities = similarities[top_indices]
            logger.info("Similar articles found using cosine similarity.")
        
        results = self.df.iloc[top_indices][["dieu", "truncated_text"]].copy()
        results["similarity"] = similarities
        return results

    # -------------------------------
    # Methods for Saving and Loading Model & State
    # -------------------------------
    def save_model(self, model_dir: str):
        """Save the Hugging Face model and tokenizer to the specified directory."""
        os.makedirs(model_dir, exist_ok=True)
        self.model.save_pretrained(model_dir)
        self.tokenizer.save_pretrained(model_dir)
        logger.info(f"Model and tokenizer saved to {model_dir}.")

    def load_model(self, model_dir: str):
        """Load the Hugging Face model and tokenizer from the specified directory."""
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)
        self.model = AutoModel.from_pretrained(model_dir)
        self.model.to(self.device)
        self.model.eval()
        logger.info(f"Model and tokenizer loaded from {model_dir}.")

    def save_state(self, state_path: str):
        """Save custom state (DataFrame and embeddings) to disk."""
        with open(state_path, "wb") as f:
            pickle.dump({"df": self.df, "embeddings": self.embeddings}, f)
        logger.info(f"State saved to {state_path}.")

    def load_state(self, state_path: str):
        """Load custom state (DataFrame and embeddings) from disk."""
        if os.path.exists(state_path):
            with open(state_path, "rb") as f:
                cache = pickle.load(f)
                self.df = cache["df"]
                self.embeddings = cache["embeddings"]
            logger.info(f"State loaded from {state_path}.")
        else:
            logger.warning(f"State file {state_path} does not exist.")

    def save_index(self, index_path: str):
        """Save the FAISS index to disk."""
        if self.index is not None and faiss is not None:
            faiss.write_index(self.index, index_path)
            logger.info(f"FAISS index saved to {index_path}.")
        else:
            logger.warning("No FAISS index available to save.")

    def load_index(self, index_path: str):
        """Load the FAISS index from disk."""
        if os.path.exists(index_path) and faiss is not None:
            self.index = faiss.read_index(index_path)
            logger.info(f"FAISS index loaded from {index_path}.")
        else:
            logger.warning(f"Index file {index_path} does not exist or FAISS is not available.")



# Cell 3: Setting Parameters and Loading Data
data_file = "sent_truncated_vbpl_legal_only.csv"  # Update path if necessary.
cache_file = "embeddings_cache.pkl"               # For caching embeddings.
index_file = "faiss_index.bin"                    # For saving/loading the FAISS index.
state_file = "custom_state.pkl"                   # For saving/loading custom state (DataFrame, embeddings).
batch_size = 32
use_faiss = True

finder = LegalDocumentFinder()

# Load and preprocess legal documents.
finder.load_data(data_file)



# Cell 4: Compute/Load Embeddings, Build FAISS Index, and Save State
if not finder.load_cached_embeddings(cache_file):
    texts = finder.df["truncated_text"].tolist()
    finder.batch_encode_texts(texts, batch_size=batch_size)
    # Save embeddings in DataFrame and cache them.
    finder.df["embedding"] = list(finder.embeddings)
    finder.cache_embeddings(cache_file)
else:
    logger.info("Using cached embeddings.")

if use_faiss:
    finder.build_index()
    finder.save_index(index_file)  # Save the FAISS index to disk.

# Save the custom state (DataFrame and embeddings) for future use.
finder.save_state(state_file)



# Cell 5: Save the Hugging Face Model and Tokenizer
model_dir = "phobert_saved_model"  # Define directory where to save the model.
finder.save_model(model_dir)



# Cell 6: Demo Search and Display Results
from IPython.display import display, HTML

demo_text = "Bi√™n b·∫£n h·ªçp H·ªôi ƒë·ªìng th√†nh vi√™n ph·∫£i ƒë∆∞·ª£c gi·ªØ nguy√™n v√† kh√¥ng ƒë∆∞·ª£c ph√©p thay ƒë·ªïi"
results = finder.find_similar_articles(demo_text, top_k=5)

print("\nüîç Top Related Articles:")
for idx, row in results.iterrows():
    print(f"\nüìò {row['dieu']} (Similarity: {row['similarity']:.2f})")
    print(f"üìù {row['truncated_text']}")

display(HTML(results.to_html(index=False)))




