import os
import logging
import pickle
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity

# importing faiss for efficient similarity search.
try:
    import faiss
except ImportError:
    faiss = None
    logging.info("Faiss is not installed; falling back to cosine similarity.")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Cell 2: LegalDocumentFinder Class Definition
class LegalDocumentFinder:
    def __init__(self, model_name: str = "vinai/phobert-base", device: str = None):
        # Set device to CUDA if available, otherwise CPU.
        self.device = torch.device(device) if device else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # Load the tokenizer and model.
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        self.embeddings = None  # Will hold the computed embeddings as a NumPy array.
        self.df = None          # The DataFrame holding the legal documents.
        self.index = None       # The FAISS index for fast similarity search (optional).

    def load_data(self, file_path: str) -> pd.DataFrame:
        """Load CSV data while handling common encoding issues."""
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='"')
            logger.info("Loaded data using utf-8-sig encoding.")
        except Exception as e:
            logger.warning(f"utf-8-sig encoding failed: {e}. Trying utf-16...")
            df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='"')
        # Remove rows with missing values and duplicates.
        df = df.dropna(subset=["truncated_text", "dieu"]).drop_duplicates(subset=["truncated_text"])
        df.reset_index(drop=True, inplace=True)
        logger.info(f"Data cleaned: {len(df)} rows.")
        self.df = df
        return df

    def batch_encode_texts(self, texts: list, batch_size: int = 32, max_length: int = 256) -> np.ndarray:
        """Encode texts in batches using the model (supports GPU)."""
        embeddings = []
        logger.info(f"Starting batch encoding for {len(texts)} texts...")
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding Batches"):
            batch_texts = texts[i: i + batch_size]
            inputs = self.tokenizer(batch_texts, return_tensors="pt", truncation=True,
                                      max_length=max_length, padding="max_length")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = self.model(**inputs)
            # Extract the [CLS] token embedding from the last hidden state.
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.append(batch_embeddings)
        embeddings = np.concatenate(embeddings, axis=0)
        logger.info(f"Completed encoding. Embeddings shape: {embeddings.shape}")
        self.embeddings = embeddings
        return embeddings

    def cache_embeddings(self, cache_path: str):
        """Cache embeddings and DataFrame to disk to save computation time."""
        with open(cache_path, "wb") as f:
            pickle.dump({"df": self.df, "embeddings": self.embeddings}, f)
        logger.info(f"Embeddings cached to {cache_path}.")

    def load_cached_embeddings(self, cache_path: str) -> bool:
        """Load cached embeddings and DataFrame from disk if available."""
        if os.path.exists(cache_path):
            with open(cache_path, "rb") as f:
                cache = pickle.load(f)
                self.df = cache["df"]
                self.embeddings = cache["embeddings"]
            logger.info(f"Loaded cached embeddings from {cache_path}.")
            return True
        return False

    def build_index(self):
        """Build a FAISS index for fast similarity search (if faiss is available)."""
        if faiss is not None and self.embeddings is not None:
            d = self.embeddings.shape[1]
            self.index = faiss.IndexFlatL2(d)
            # Normalize embeddings for cosine-similarity approximation.
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings.astype(np.float32))
            logger.info(f"FAISS index built with {self.index.ntotal} vectors.")
        else:
            logger.info("FAISS not available or embeddings not computed; skipping index build.")

    def encode_text(self, text: str, max_length: int = 256) -> np.ndarray:
        """Encode a single text into its vector representation."""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True,
                                max_length=max_length, padding="max_length")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

    def find_similar_articles(self, input_text: str, top_k: int = 5) -> pd.DataFrame:
        """Retrieve the top_k similar legal documents to the input text."""
        input_embedding = self.encode_text(input_text).reshape(1, -1)
        input_norm = input_embedding / np.linalg.norm(input_embedding, axis=1, keepdims=True)
        
        if self.index is not None:
            # Use FAISS index for efficient search.
            input_embedding_norm = input_norm.astype(np.float32)
            distances, indices = self.index.search(input_embedding_norm, top_k)
            # Compute approximate cosine similarities from L2 distances.
            similarities = 1 - distances.flatten() / 2  
            top_indices = indices.flatten()
            logger.info("Similar articles found using FAISS.")
        else:
            # Fall back to computing cosine similarity via scikit-learn.
            matrix = np.stack(self.df["embedding"].values)
            similarities = cosine_similarity(input_embedding, matrix).flatten()
            top_indices = similarities.argsort()[::-1][:top_k]
            similarities = similarities[top_indices]
            logger.info("Similar articles found using cosine similarity.")
        
        results = self.df.iloc[top_indices][["dieu", "truncated_text"]].copy()
        results["similarity"] = similarities
        return results



# Cell 3: Setting Parameters and Loading Data
# Parameters (adjust these as necessary)
data_file = "sent_truncated_vbpl_legal_only.csv"  # Update if your CSV file is located elsewhere.
cache_file = "embeddings_cache.pkl"               # Change cache file path if desired.
batch_size = 32                                   # You can adjust batch size based on available GPU/CPU memory.
use_faiss = True                                  # Set True to build a FAISS index for faster search.

# Create an instance of LegalDocumentFinder.
finder = LegalDocumentFinder()

# Load and preprocess your legal documents.
df = finder.load_data(data_file)



# Cell 4: Compute or Load Embeddings and Build FAISS Index
if not finder.load_cached_embeddings(cache_file):
    texts = finder.df["truncated_text"].tolist()
    embeddings = finder.batch_encode_texts(texts, batch_size=batch_size)
    # Save the embeddings in the DataFrame and cache them.
    finder.df["embedding"] = list(embeddings)
    finder.cache_embeddings(cache_file)
else:
    logger.info("Using cached embeddings.")

# Build FAISS index if enabled.
if use_faiss:
    finder.build_index()



# Cell 5: Demo Search and Display Results
from IPython.display import display, HTML

# Define your demo query text.
demo_text = "Bi√™n b·∫£n h·ªçp H·ªôi ƒë·ªìng th√†nh vi√™n ph·∫£i ƒë∆∞·ª£c gi·ªØ nguy√™n v√† kh√¥ng ƒë∆∞·ª£c ph√©p thay ƒë·ªïi"

# Get the top 5 similar legal documents.
results = finder.find_similar_articles(demo_text, top_k=5)

# Print results in a nicely formatted way.
print("\nüîç Top Related Articles:")
for idx, row in results.iterrows():
    print(f"\nüìò {row['dieu']} (Similarity: {row['similarity']:.2f})")
    print(f"üìù {row['truncated_text']}")

# Optional: Display results as a formatted HTML table.
display(HTML(results.to_html(index=False)))




