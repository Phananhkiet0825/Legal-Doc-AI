{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82842f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8-sig failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "üîÑ Encoding all ƒêi·ªÅu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [00:34<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done encoding.\n",
      "\n",
      "üîç Top Related Articles:\n",
      "\n",
      "üìò ƒêi·ªÅu 159 (Similarity: 0.72)\n",
      "üìù H·ª£p ƒë·ªìng u·ª∑ th√°c mua b√°n h√†ng ho√° ph·∫£i ƒë∆∞·ª£c l·∫≠p th√†nh vƒÉn b·∫£n ho·∫∑c b·∫±ng h√¨nh th·ª©c kh√°c c√≥ gi√° tr·ªã ph√°p l√Ω t∆∞∆°ng ƒë∆∞∆°ng.\n",
      "\n",
      "üìò ƒêi·ªÅu 251 (Similarity: 0.70)\n",
      "üìù H·ª£p ƒë·ªìng d·ªãch v·ª• qu√° c·∫£nh ph·∫£i ƒë∆∞·ª£c l·∫≠p th√†nh vƒÉn b·∫£n ho·∫∑c b·∫±ng h√¨nh th·ª©c kh√°c c√≥ gi√° tr·ªã ph√°p l√Ω t∆∞∆°ng ƒë∆∞∆°ng.\n",
      "\n",
      "üìò ƒêi·ªÅu 179 (Similarity: 0.69)\n",
      "üìù H·ª£p ƒë·ªìng gia c√¥ng ph·∫£i ƒë∆∞·ª£c l·∫≠p th√†nh vƒÉn b·∫£n ho·∫∑c b·∫±ng h√¨nh th·ª©c kh√°c c√≥ gi√° tr·ªã ph√°p l√Ω t∆∞∆°ng ƒë∆∞∆°ng.\n",
      "\n",
      "üìò ƒêi·ªÅu 362 (Similarity: 0.69)\n",
      "üìù H√¨nh th·ª©c b·∫£o l√£nh Vi·ªác b·∫£o l√£nh ph·∫£i ƒë∆∞·ª£c l·∫≠p th√†nh vƒÉn b·∫£n, c√≥ th·ªÉ l·∫≠p th√†nh vƒÉn b·∫£n ri√™ng ho·∫∑c ghi trong h·ª£p ƒë·ªìng ch√≠nh. Trong tr∆∞·ªùng h·ª£p ph√°p lu·∫≠t c√≥ quy ƒë·ªãnh th√¨ vƒÉn b·∫£n b·∫£o l√£nh ph·∫£i ƒë∆∞·ª£c c√¥ng ch·ª©ng ho·∫∑c ch·ª©ng th·ª±c.\n",
      "\n",
      "üìò ƒêi·ªÅu 142 (Similarity: 0.69)\n",
      "üìù H·ª£p ƒë·ªìng ƒë·∫°i di·ªán cho th∆∞∆°ng nh√¢n ph·∫£i ƒë∆∞·ª£c l·∫≠p th√†nh vƒÉn b·∫£n ho·∫∑c b·∫±ng h√¨nh th·ª©c kh√°c c√≥ gi√° tr·ªã ph√°p l√Ω t∆∞∆°ng ƒë∆∞∆°ng.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 2: Load data with appropriate encoding\n",
    "file_path = \"sent_truncated_vbpl_legal_only.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='\"')\n",
    "except Exception as e:\n",
    "    print(\"utf-8-sig failed:\", e)\n",
    "    df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='\"')\n",
    "\n",
    "# Drop rows with missing values and duplicates in \"truncated_text\"\n",
    "df = df.dropna(subset=[\"truncated_text\", \"dieu\"]).drop_duplicates(subset=[\"truncated_text\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 3: Load PhoBERT model & tokenizer, and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Step 4: Define batch encoding function using GPU\n",
    "def batch_encode_texts(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Batches\"):\n",
    "        batch_texts = texts[i: i + batch_size]\n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, max_length=256, padding=\"max_length\")\n",
    "        # Move input tensors to the GPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Extract the CLS token embedding from the last hidden state\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Step 5: Encode all legal texts in batches and cache embeddings\n",
    "print(\"üîÑ Encoding all ƒêi·ªÅu...\")\n",
    "all_texts = df[\"truncated_text\"].tolist()\n",
    "embeddings = batch_encode_texts(all_texts, batch_size=32)\n",
    "df[\"embedding\"] = list(embeddings)  # Save each embedding as a numpy array\n",
    "print(\"‚úÖ Done encoding.\")\n",
    "\n",
    "# Step 6: Helper function for single text encoding (for inference)\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=\"max_length\")\n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Return the embedding for the [CLS] token\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "# Step 7: Function to find top similar articles using cosine similarity\n",
    "def find_similar_articles(input_text, top_k=5):\n",
    "    input_embedding = encode_text(input_text).reshape(1, -1)\n",
    "    matrix = np.stack(df[\"embedding\"].values)\n",
    "    similarities = cosine_similarity(input_embedding, matrix).flatten()\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "    \n",
    "    results = df.iloc[top_indices][[\"dieu\", \"truncated_text\"]].copy()\n",
    "    results[\"similarity\"] = similarities[top_indices]\n",
    "    return results\n",
    "\n",
    "# Step 8: Demo Input & Print top similar articles\n",
    "demo_text = \"Bi√™n b·∫£n h·ªçp H·ªôi ƒë·ªìng th√†nh vi√™n ph·∫£i ƒë∆∞·ª£c gi·ªØ nguy√™n v√† kh√¥ng ƒë∆∞·ª£c ph√©p thay ƒë·ªïi\"\n",
    "results = find_similar_articles(demo_text)\n",
    "print(\"\\nüîç Top Related Articles:\")\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"\\nüìò {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "    print(f\"üìù {row['truncated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d21d7f-934a-4948-ab2f-fc63a2afed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%` not found.\n"
     ]
    }
   ],
   "source": [
    " %% [code]\n",
    "# Step 8: Demo Input & Print top similar articles\n",
    "demo_text = \"\"\"Bi√™n b·∫£n h·ªçp H·ªôi ƒë·ªìng th√†nh vi√™n ph·∫£i ƒë∆∞·ª£c gi·ªØ nguy√™n v√† kh√¥ng ƒë∆∞·ª£c ph√©p thay ƒë·ªïi...\"\"\"\n",
    "results = find_similar_articles(demo_text)\n",
    "print(\"\\nüîç Top Related Articles:\")\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"\\nüìò {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "    print(f\"üìù {row['truncated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb733513-753f-41c5-8dec-1cc03b9eb7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
