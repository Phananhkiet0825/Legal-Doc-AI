{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5b3351-b305-4368-b6fa-3cde9aa56a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Dependencies and Logging Configuration\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Try importing faiss for efficient similarity search.\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    faiss = None\n",
    "    logging.info(\"Faiss is not installed; falling back to cosine similarity.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96270d19-8689-45cc-874d-e99a792f7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: LegalDocumentFinder Class Definition with Save/Load Methods\n",
    "class LegalDocumentFinder:\n",
    "    def __init__(self, model_name: str = \"vinai/phobert-base\", device: str = None):\n",
    "        # Set device to CUDA if available, otherwise CPU.\n",
    "        self.device = torch.device(device) if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load the tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.embeddings = None  # Will hold computed embeddings as a NumPy array.\n",
    "        self.df = None          # DataFrame holding the legal documents.\n",
    "        self.index = None       # FAISS index for fast similarity search (optional).\n",
    "\n",
    "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data while handling common encoding issues.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='\"')\n",
    "            logger.info(\"Loaded data using utf-8-sig encoding.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"utf-8-sig encoding failed: {e}. Trying utf-16...\")\n",
    "            df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='\"')\n",
    "        # Remove rows with missing values and duplicates.\n",
    "        df = df.dropna(subset=[\"truncated_text\", \"dieu\"]).drop_duplicates(subset=[\"truncated_text\"])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"Data cleaned: {len(df)} rows.\")\n",
    "        self.df = df\n",
    "        return df\n",
    "\n",
    "    def batch_encode_texts(self, texts: list, batch_size: int = 32, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode texts in batches using GPU if available.\"\"\"\n",
    "        embeddings = []\n",
    "        logger.info(f\"Starting batch encoding for {len(texts)} texts...\")\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch_texts = texts[i: i + batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                                      max_length=max_length, padding=\"max_length\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extract the [CLS] token embedding from the last hidden state.\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        logger.info(f\"Completed encoding. Embeddings shape: {embeddings.shape}\")\n",
    "        self.embeddings = embeddings\n",
    "        return embeddings\n",
    "\n",
    "    def cache_embeddings(self, cache_path: str):\n",
    "        \"\"\"Cache embeddings and DataFrame to disk.\"\"\"\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump({\"df\": self.df, \"embeddings\": self.embeddings}, f)\n",
    "        logger.info(f\"Embeddings cached to {cache_path}.\")\n",
    "\n",
    "    def load_cached_embeddings(self, cache_path: str) -> bool:\n",
    "        \"\"\"Load cached embeddings and DataFrame from disk if available.\"\"\"\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "                self.df = cache[\"df\"]\n",
    "                self.embeddings = cache[\"embeddings\"]\n",
    "            logger.info(f\"Loaded cached embeddings from {cache_path}.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"Build a FAISS index for fast similarity search (if available).\"\"\"\n",
    "        if faiss is not None and self.embeddings is not None:\n",
    "            d = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(d)\n",
    "            # Normalize embeddings for cosine-similarity approximation.\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "            self.index.add(self.embeddings.astype(np.float32))\n",
    "            logger.info(f\"FAISS index built with {self.index.ntotal} vectors.\")\n",
    "        else:\n",
    "            logger.info(\"FAISS not available or embeddings not computed; skipping index build.\")\n",
    "\n",
    "    def encode_text(self, text: str, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode a single text into its vector representation.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                                max_length=max_length, padding=\"max_length\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "    def find_similar_articles(self, input_text: str, top_k: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Retrieve the top_k similar legal documents for the input text.\"\"\"\n",
    "        input_embedding = self.encode_text(input_text).reshape(1, -1)\n",
    "        input_norm = input_embedding / np.linalg.norm(input_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        if self.index is not None:\n",
    "            # Use FAISS index for efficient search.\n",
    "            input_embedding_norm = input_norm.astype(np.float32)\n",
    "            distances, indices = self.index.search(input_embedding_norm, top_k)\n",
    "            # Approximate cosine similarity from L2 distances.\n",
    "            similarities = 1 - distances.flatten() / 2  \n",
    "            top_indices = indices.flatten()\n",
    "            logger.info(\"Similar articles found using FAISS.\")\n",
    "        else:\n",
    "            # Fall back to cosine similarity using scikit-learn.\n",
    "            matrix = np.stack(self.df[\"embedding\"].values)\n",
    "            similarities = cosine_similarity(input_embedding, matrix).flatten()\n",
    "            top_indices = similarities.argsort()[::-1][:top_k]\n",
    "            similarities = similarities[top_indices]\n",
    "            logger.info(\"Similar articles found using cosine similarity.\")\n",
    "        \n",
    "        results = self.df.iloc[top_indices][[\"dieu\", \"truncated_text\"]].copy()\n",
    "        results[\"similarity\"] = similarities\n",
    "        return results\n",
    "\n",
    "    # -------------------------------\n",
    "    # Methods for Saving and Loading Model & State\n",
    "    # -------------------------------\n",
    "    def save_model(self, model_dir: str):\n",
    "        \"\"\"Save the Hugging Face model and tokenizer to the specified directory.\"\"\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        logger.info(f\"Model and tokenizer saved to {model_dir}.\")\n",
    "\n",
    "    def load_model(self, model_dir: str):\n",
    "        \"\"\"Load the Hugging Face model and tokenizer from the specified directory.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_dir)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        logger.info(f\"Model and tokenizer loaded from {model_dir}.\")\n",
    "\n",
    "    def save_state(self, state_path: str):\n",
    "        \"\"\"Save custom state (DataFrame and embeddings) to disk.\"\"\"\n",
    "        with open(state_path, \"wb\") as f:\n",
    "            pickle.dump({\"df\": self.df, \"embeddings\": self.embeddings}, f)\n",
    "        logger.info(f\"State saved to {state_path}.\")\n",
    "\n",
    "    def load_state(self, state_path: str):\n",
    "        \"\"\"Load custom state (DataFrame and embeddings) from disk.\"\"\"\n",
    "        if os.path.exists(state_path):\n",
    "            with open(state_path, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "                self.df = cache[\"df\"]\n",
    "                self.embeddings = cache[\"embeddings\"]\n",
    "            logger.info(f\"State loaded from {state_path}.\")\n",
    "        else:\n",
    "            logger.warning(f\"State file {state_path} does not exist.\")\n",
    "\n",
    "    def save_index(self, index_path: str):\n",
    "        \"\"\"Save the FAISS index to disk.\"\"\"\n",
    "        if self.index is not None and faiss is not None:\n",
    "            faiss.write_index(self.index, index_path)\n",
    "            logger.info(f\"FAISS index saved to {index_path}.\")\n",
    "        else:\n",
    "            logger.warning(\"No FAISS index available to save.\")\n",
    "\n",
    "    def load_index(self, index_path: str):\n",
    "        \"\"\"Load the FAISS index from disk.\"\"\"\n",
    "        if os.path.exists(index_path) and faiss is not None:\n",
    "            self.index = faiss.read_index(index_path)\n",
    "            logger.info(f\"FAISS index loaded from {index_path}.\")\n",
    "        else:\n",
    "            logger.warning(f\"Index file {index_path} does not exist or FAISS is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049b56cf-8b47-41fb-b29b-f28d3f30200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:utf-8-sig encoding failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte. Trying utf-16...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>so_hieu</th>\n",
       "      <th>dieu</th>\n",
       "      <th>prefix</th>\n",
       "      <th>truncated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Điều 1</td>\n",
       "      <td>Điều 1 luật 59/2020/QH14 doanh nghiệp</td>\n",
       "      <td>Phạm vi điều chỉnh Luật này quy định về việc t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Điều 2</td>\n",
       "      <td>Điều 2 luật 59/2020/QH14 doanh nghiệp</td>\n",
       "      <td>Đối tượng áp dụng 1. Doanh nghiệp. 2. Cơ quan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Điều 3</td>\n",
       "      <td>Điều 3 luật 59/2020/QH14 doanh nghiệp</td>\n",
       "      <td>Áp dụng Luật Doanh nghiệp và luật khác Trường ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Điều 4</td>\n",
       "      <td>Điều 4 luật 59/2020/QH14 doanh nghiệp</td>\n",
       "      <td>Giải thích từ ngữ Trong Luật này, các từ ngữ d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Điều 4</td>\n",
       "      <td>Điều 4 luật 59/2020/QH14 doanh nghiệp</td>\n",
       "      <td>11. Doanh nghiệp nhà nước bao gồm các doanh ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Điều 84</td>\n",
       "      <td>Điều 84 luật 24-L/CTN on environment</td>\n",
       "      <td>Người nước ngoài có hành vi vi phạm pháp luật ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Điều 86</td>\n",
       "      <td>Điều 86 luật 24-L/CTN on environment</td>\n",
       "      <td>Người nào lợi dụng chức vụ, quyền hạn hoặc vượ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Điều 87</td>\n",
       "      <td>Điều 87 luật 24-L/CTN on environment</td>\n",
       "      <td>Người nào có hành vi vi phạm pháp luật đất đai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Điều 88</td>\n",
       "      <td>Điều 88 luật 24-L/CTN on environment</td>\n",
       "      <td>Luật này thay thế Luật đất đai đã được Quốc hộ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Điều 89</td>\n",
       "      <td>Điều 89 luật 24-L/CTN on environment</td>\n",
       "      <td>Chính phủ quy định chi tiết thi hành Luật này....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9254 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           so_hieu     dieu                                 prefix  \\\n",
       "0     59/2020/QH14   Điều 1  Điều 1 luật 59/2020/QH14 doanh nghiệp   \n",
       "1     59/2020/QH14   Điều 2  Điều 2 luật 59/2020/QH14 doanh nghiệp   \n",
       "2     59/2020/QH14   Điều 3  Điều 3 luật 59/2020/QH14 doanh nghiệp   \n",
       "3     59/2020/QH14   Điều 4  Điều 4 luật 59/2020/QH14 doanh nghiệp   \n",
       "4     59/2020/QH14   Điều 4  Điều 4 luật 59/2020/QH14 doanh nghiệp   \n",
       "...            ...      ...                                    ...   \n",
       "9249      24-L/CTN  Điều 84   Điều 84 luật 24-L/CTN on environment   \n",
       "9250      24-L/CTN  Điều 86   Điều 86 luật 24-L/CTN on environment   \n",
       "9251      24-L/CTN  Điều 87   Điều 87 luật 24-L/CTN on environment   \n",
       "9252      24-L/CTN  Điều 88   Điều 88 luật 24-L/CTN on environment   \n",
       "9253      24-L/CTN  Điều 89   Điều 89 luật 24-L/CTN on environment   \n",
       "\n",
       "                                         truncated_text  \n",
       "0     Phạm vi điều chỉnh Luật này quy định về việc t...  \n",
       "1     Đối tượng áp dụng 1. Doanh nghiệp. 2. Cơ quan,...  \n",
       "2     Áp dụng Luật Doanh nghiệp và luật khác Trường ...  \n",
       "3     Giải thích từ ngữ Trong Luật này, các từ ngữ d...  \n",
       "4     11. Doanh nghiệp nhà nước bao gồm các doanh ng...  \n",
       "...                                                 ...  \n",
       "9249  Người nước ngoài có hành vi vi phạm pháp luật ...  \n",
       "9250  Người nào lợi dụng chức vụ, quyền hạn hoặc vượ...  \n",
       "9251  Người nào có hành vi vi phạm pháp luật đất đai...  \n",
       "9252  Luật này thay thế Luật đất đai đã được Quốc hộ...  \n",
       "9253  Chính phủ quy định chi tiết thi hành Luật này....  \n",
       "\n",
       "[9254 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Setting Parameters and Loading Data\n",
    "data_file = \"sent_truncated_vbpl_legal_only.csv\"  # Update path if necessary.\n",
    "cache_file = \"embeddings_cache.pkl\"               # For caching embeddings.\n",
    "index_file = \"faiss_index.bin\"                    # For saving/loading the FAISS index.\n",
    "state_file = \"custom_state.pkl\"                   # For saving/loading custom state (DataFrame, embeddings).\n",
    "batch_size = 32\n",
    "use_faiss = True\n",
    "\n",
    "finder = LegalDocumentFinder()\n",
    "\n",
    "# Load and preprocess legal documents.\n",
    "finder.load_data(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91291563-ab15-45da-8fde-5bb40e056ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:No FAISS index available to save.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Compute/Load Embeddings, Build FAISS Index, and Save State\n",
    "if not finder.load_cached_embeddings(cache_file):\n",
    "    texts = finder.df[\"truncated_text\"].tolist()\n",
    "    finder.batch_encode_texts(texts, batch_size=batch_size)\n",
    "    # Save embeddings in DataFrame and cache them.\n",
    "    finder.df[\"embedding\"] = list(finder.embeddings)\n",
    "    finder.cache_embeddings(cache_file)\n",
    "else:\n",
    "    logger.info(\"Using cached embeddings.\")\n",
    "\n",
    "if use_faiss:\n",
    "    finder.build_index()\n",
    "    finder.save_index(index_file)  # Save the FAISS index to disk.\n",
    "\n",
    "# Save the custom state (DataFrame and embeddings) for future use.\n",
    "finder.save_state(state_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29686774-ad39-40b0-967b-487d4c566925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save the Hugging Face Model and Tokenizer\n",
    "model_dir = \"phobert_saved_model\"  # Define directory where to save the model.\n",
    "finder.save_model(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2755049f-4e5a-4de8-a620-ee04f84d0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top Related Articles:\n",
      "\n",
      "📘 Điều 39 (Similarity: 0.68)\n",
      "📝 Tù chung thân Tù chung thân là hình phạt tù không thời hạn được áp dụng đối với người phạm tội đặc biệt nghiêm trọng, nhưng chưa đến mức bị xử phạt tử hình. Không áp dụng hình phạt tù chung thân đối với người dưới 18 tuổi phạm tội.\n",
      "\n",
      "📘 Điều 347 (Similarity: 0.67)\n",
      "📝 Tội vi phạm quy định về xuất cảnh, nhập cảnh; tội ở lại Việt Nam trái phép Người nào xuất cảnh, nhập cảnh trái phép hoặc ở lại Việt Nam trái phép, đã bị xử phạt vi phạm hành chính về hành vi này mà còn vi phạm, thì bị phạt tiền từ 5.000.000 đồng đến 50.000.000 đồng hoặc phạt tù từ 06 tháng đến 03 năm.\n",
      "\n",
      "📘 Điều 130 (Similarity: 0.67)\n",
      "📝 Tội xâm phạm quyền bình đẳng của phụ nữ Người nào dùng vũ lực hoặc có hành vi nghiêm trọng khác cản trở phụ nữ tham gia hoạt động chính trị, kinh tế, khoa học, văn hoá, xã hội, thì bị phạt cảnh cáo, cải tạo không giam giữ đến một năm hoặc phạt tù từ ba tháng đến một năm.\n",
      "\n",
      "📘 Điều 94 (Similarity: 0.66)\n",
      "📝 Tội giết con mới đẻ Người mẹ nào do ảnh hưởng nặng nề của tư tưởng lạc hậu hoặc trong hoàn cảnh khách quan đặc biệt mà giết con mới đẻ hoặc vứt bỏ đứa trẻ đó dẫn đến hậu quả đứa trẻ chết, thì bị phạt cải tạo không giam giữ đến hai năm hoặc phạt tù từ ba tháng đến hai năm.\n",
      "\n",
      "📘 Điều 152 (Similarity: 0.66)\n",
      "📝 Tội từ chối hoặc trốn tránh nghĩa vụ cấp dưỡng Người nào có nghĩa vụ cấp dưỡng và có khả năng thực tế để thực hiện việc cấp dưỡng đối với người mà mình có nghĩa vụ cấp dưỡng theo quy định của pháp luật mà cố ý từ chối hoặc trốn tránh nghĩa vụ cấp dưỡng gây hậu quả nghiêm trọng hoặc đã bị xử phạt hành chính về hành vi này mà còn vi phạm, thì bị phạt cảnh cáo, cải tạo không giam giữ đến hai năm hoặc phạt tù từ ba tháng đến hai năm.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dieu</th>\n",
       "      <th>truncated_text</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Điều 39</td>\n",
       "      <td>Tù chung thân Tù chung thân là hình phạt tù không thời hạn được áp dụng đối với người phạm tội đặc biệt nghiêm trọng, nhưng chưa đến mức bị xử phạt tử hình. Không áp dụng hình phạt tù chung thân đối với người dưới 18 tuổi phạm tội.</td>\n",
       "      <td>0.677016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Điều 347</td>\n",
       "      <td>Tội vi phạm quy định về xuất cảnh, nhập cảnh; tội ở lại Việt Nam trái phép Người nào xuất cảnh, nhập cảnh trái phép hoặc ở lại Việt Nam trái phép, đã bị xử phạt vi phạm hành chính về hành vi này mà còn vi phạm, thì bị phạt tiền từ 5.000.000 đồng đến 50.000.000 đồng hoặc phạt tù từ 06 tháng đến 03 năm.</td>\n",
       "      <td>0.673796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Điều 130</td>\n",
       "      <td>Tội xâm phạm quyền bình đẳng của phụ nữ Người nào dùng vũ lực hoặc có hành vi nghiêm trọng khác cản trở phụ nữ tham gia hoạt động chính trị, kinh tế, khoa học, văn hoá, xã hội, thì bị phạt cảnh cáo, cải tạo không giam giữ đến một năm hoặc phạt tù từ ba tháng đến một năm.</td>\n",
       "      <td>0.672321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Điều 94</td>\n",
       "      <td>Tội giết con mới đẻ Người mẹ nào do ảnh hưởng nặng nề của tư tưởng lạc hậu hoặc trong hoàn cảnh khách quan đặc biệt mà giết con mới đẻ hoặc vứt bỏ đứa trẻ đó dẫn đến hậu quả đứa trẻ chết, thì bị phạt cải tạo không giam giữ đến hai năm hoặc phạt tù từ ba tháng đến hai năm.</td>\n",
       "      <td>0.663199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Điều 152</td>\n",
       "      <td>Tội từ chối hoặc trốn tránh nghĩa vụ cấp dưỡng Người nào có nghĩa vụ cấp dưỡng và có khả năng thực tế để thực hiện việc cấp dưỡng đối với người mà mình có nghĩa vụ cấp dưỡng theo quy định của pháp luật mà cố ý từ chối hoặc trốn tránh nghĩa vụ cấp dưỡng gây hậu quả nghiêm trọng hoặc đã bị xử phạt hành chính về hành vi này mà còn vi phạm, thì bị phạt cảnh cáo, cải tạo không giam giữ đến hai năm hoặc phạt tù từ ba tháng đến hai năm.</td>\n",
       "      <td>0.661434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: Demo Search and Display Results\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "demo_text = \"vi phạm pháp luật bị phạt tù  \"\n",
    "results = finder.find_similar_articles(demo_text, top_k=5)\n",
    "\n",
    "print(\"\\n🔍 Top Related Articles:\")\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"\\n📘 {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "    print(f\"📝 {row['truncated_text']}\")\n",
    "\n",
    "display(HTML(results.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d7d54f-1b60-4b8c-8154-52e313ca0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name phobert_saved_model. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Load the dataset\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_truncated_vbpl_legal_only.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ✅ Check required columns\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncated_text\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncated_text\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column in the CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U sentencepiece\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load PhoBERT model\n",
    "model_name = \"phobert_saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def encode_phobert(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# 2. Load legal dataset with proper encoding\n",
    "df = pd.read_csv(\"sent_truncated_vbpl_legal_only.csv\", encoding=\"utf-16\")  # Change to ISO-8859-1 if needed\n",
    "assert 'truncated_text' in df.columns\n",
    "\n",
    "# 3. Compute embeddings for all legal documents\n",
    "print(\"🔄 Encoding all legal texts...\")\n",
    "corpus = df[\"truncated_text\"].astype(str).tolist()\n",
    "corpus_embeddings = encode_phobert(corpus)\n",
    "\n",
    "# 4. Input text to find similar articles\n",
    "demo_text = \"Biên bản họp Hội đồng thành viên phải được giữ nguyên và không được phép thay đổi\"\n",
    "query_embedding = encode_phobert([demo_text])\n",
    "\n",
    "# 5. Compute cosine similarity\n",
    "similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "top_k = 5\n",
    "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "# 6. Show results\n",
    "print(\"\\n🔍 Top Related Articles:\")\n",
    "results = []\n",
    "for idx in top_indices:\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"\\n📘 Điều: {row['dieu']} (Similarity: {similarities[idx]:.2f})\")\n",
    "    print(f\"📝 {row['truncated_text']}\")\n",
    "    results.append({\n",
    "        \"Điều\": row[\"dieu\"],\n",
    "        \"Văn bản\": row[\"truncated_text\"],\n",
    "        \"Độ tương đồng\": round(similarities[idx], 2)\n",
    "    })\n",
    "\n",
    "# 7. Display as HTML table\n",
    "display(HTML(pd.DataFrame(results).to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34cf18-59c2-43ec-a6cb-91976e452cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
