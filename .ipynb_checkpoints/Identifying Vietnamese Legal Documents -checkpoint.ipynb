{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5b3351-b305-4368-b6fa-3cde9aa56a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Dependencies and Logging Configuration\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Try importing faiss for efficient similarity search.\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    faiss = None\n",
    "    logging.info(\"Faiss is not installed; falling back to cosine similarity.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96270d19-8689-45cc-874d-e99a792f7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: LegalDocumentFinder Class Definition with Save/Load Methods\n",
    "class LegalDocumentFinder:\n",
    "    def __init__(self, model_name: str = \"vinai/phobert-base\", device: str = None):\n",
    "        # Set device to CUDA if available, otherwise CPU.\n",
    "        self.device = torch.device(device) if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load the tokenizer and model.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.embeddings = None  # Will hold computed embeddings as a NumPy array.\n",
    "        self.df = None          # DataFrame holding the legal documents.\n",
    "        self.index = None       # FAISS index for fast similarity search (optional).\n",
    "\n",
    "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data while handling common encoding issues.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig', sep=',', quotechar='\"')\n",
    "            logger.info(\"Loaded data using utf-8-sig encoding.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"utf-8-sig encoding failed: {e}. Trying utf-16...\")\n",
    "            df = pd.read_csv(file_path, encoding='utf-16', sep=',', quotechar='\"')\n",
    "        # Remove rows with missing values and duplicates.\n",
    "        df = df.dropna(subset=[\"truncated_text\", \"dieu\"]).drop_duplicates(subset=[\"truncated_text\"])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        logger.info(f\"Data cleaned: {len(df)} rows.\")\n",
    "        self.df = df\n",
    "        return df\n",
    "\n",
    "    def batch_encode_texts(self, texts: list, batch_size: int = 32, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode texts in batches using GPU if available.\"\"\"\n",
    "        embeddings = []\n",
    "        logger.info(f\"Starting batch encoding for {len(texts)} texts...\")\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Batches\"):\n",
    "            batch_texts = texts[i: i + batch_size]\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                                      max_length=max_length, padding=\"max_length\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extract the [CLS] token embedding from the last hidden state.\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        logger.info(f\"Completed encoding. Embeddings shape: {embeddings.shape}\")\n",
    "        self.embeddings = embeddings\n",
    "        return embeddings\n",
    "\n",
    "    def cache_embeddings(self, cache_path: str):\n",
    "        \"\"\"Cache embeddings and DataFrame to disk.\"\"\"\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump({\"df\": self.df, \"embeddings\": self.embeddings}, f)\n",
    "        logger.info(f\"Embeddings cached to {cache_path}.\")\n",
    "\n",
    "    def load_cached_embeddings(self, cache_path: str) -> bool:\n",
    "        \"\"\"Load cached embeddings and DataFrame from disk if available.\"\"\"\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "                self.df = cache[\"df\"]\n",
    "                self.embeddings = cache[\"embeddings\"]\n",
    "            logger.info(f\"Loaded cached embeddings from {cache_path}.\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"Build a FAISS index for fast similarity search (if available).\"\"\"\n",
    "        if faiss is not None and self.embeddings is not None:\n",
    "            d = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(d)\n",
    "            # Normalize embeddings for cosine-similarity approximation.\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "            self.index.add(self.embeddings.astype(np.float32))\n",
    "            logger.info(f\"FAISS index built with {self.index.ntotal} vectors.\")\n",
    "        else:\n",
    "            logger.info(\"FAISS not available or embeddings not computed; skipping index build.\")\n",
    "\n",
    "    def encode_text(self, text: str, max_length: int = 256) -> np.ndarray:\n",
    "        \"\"\"Encode a single text into its vector representation.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                                max_length=max_length, padding=\"max_length\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "    def find_similar_articles(self, input_text: str, top_k: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Retrieve the top_k similar legal documents for the input text.\"\"\"\n",
    "        input_embedding = self.encode_text(input_text).reshape(1, -1)\n",
    "        input_norm = input_embedding / np.linalg.norm(input_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        if self.index is not None:\n",
    "            # Use FAISS index for efficient search.\n",
    "            input_embedding_norm = input_norm.astype(np.float32)\n",
    "            distances, indices = self.index.search(input_embedding_norm, top_k)\n",
    "            # Approximate cosine similarity from L2 distances.\n",
    "            similarities = 1 - distances.flatten() / 2  \n",
    "            top_indices = indices.flatten()\n",
    "            logger.info(\"Similar articles found using FAISS.\")\n",
    "        else:\n",
    "            # Fall back to cosine similarity using scikit-learn.\n",
    "            matrix = np.stack(self.df[\"embedding\"].values)\n",
    "            similarities = cosine_similarity(input_embedding, matrix).flatten()\n",
    "            top_indices = similarities.argsort()[::-1][:top_k]\n",
    "            similarities = similarities[top_indices]\n",
    "            logger.info(\"Similar articles found using cosine similarity.\")\n",
    "        \n",
    "        results = self.df.iloc[top_indices][[\"dieu\", \"truncated_text\"]].copy()\n",
    "        results[\"similarity\"] = similarities\n",
    "        return results\n",
    "\n",
    "    # -------------------------------\n",
    "    # Methods for Saving and Loading Model & State\n",
    "    # -------------------------------\n",
    "    def save_model(self, model_dir: str):\n",
    "        \"\"\"Save the Hugging Face model and tokenizer to the specified directory.\"\"\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        logger.info(f\"Model and tokenizer saved to {model_dir}.\")\n",
    "\n",
    "    def load_model(self, model_dir: str):\n",
    "        \"\"\"Load the Hugging Face model and tokenizer from the specified directory.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_dir)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        logger.info(f\"Model and tokenizer loaded from {model_dir}.\")\n",
    "\n",
    "    def save_state(self, state_path: str):\n",
    "        \"\"\"Save custom state (DataFrame and embeddings) to disk.\"\"\"\n",
    "        with open(state_path, \"wb\") as f:\n",
    "            pickle.dump({\"df\": self.df, \"embeddings\": self.embeddings}, f)\n",
    "        logger.info(f\"State saved to {state_path}.\")\n",
    "\n",
    "    def load_state(self, state_path: str):\n",
    "        \"\"\"Load custom state (DataFrame and embeddings) from disk.\"\"\"\n",
    "        if os.path.exists(state_path):\n",
    "            with open(state_path, \"rb\") as f:\n",
    "                cache = pickle.load(f)\n",
    "                self.df = cache[\"df\"]\n",
    "                self.embeddings = cache[\"embeddings\"]\n",
    "            logger.info(f\"State loaded from {state_path}.\")\n",
    "        else:\n",
    "            logger.warning(f\"State file {state_path} does not exist.\")\n",
    "\n",
    "    def save_index(self, index_path: str):\n",
    "        \"\"\"Save the FAISS index to disk.\"\"\"\n",
    "        if self.index is not None and faiss is not None:\n",
    "            faiss.write_index(self.index, index_path)\n",
    "            logger.info(f\"FAISS index saved to {index_path}.\")\n",
    "        else:\n",
    "            logger.warning(\"No FAISS index available to save.\")\n",
    "\n",
    "    def load_index(self, index_path: str):\n",
    "        \"\"\"Load the FAISS index from disk.\"\"\"\n",
    "        if os.path.exists(index_path) and faiss is not None:\n",
    "            self.index = faiss.read_index(index_path)\n",
    "            logger.info(f\"FAISS index loaded from {index_path}.\")\n",
    "        else:\n",
    "            logger.warning(f\"Index file {index_path} does not exist or FAISS is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049b56cf-8b47-41fb-b29b-f28d3f30200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:utf-8-sig encoding failed: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte. Trying utf-16...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>so_hieu</th>\n",
       "      <th>dieu</th>\n",
       "      <th>prefix</th>\n",
       "      <th>truncated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Äiá»u 1</td>\n",
       "      <td>Äiá»u 1 luáº­t 59/2020/QH14 doanh nghiá»‡p</td>\n",
       "      <td>Pháº¡m vi Ä‘iá»u chá»‰nh Luáº­t nÃ y quy Ä‘á»‹nh vá» viá»‡c t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Äiá»u 2</td>\n",
       "      <td>Äiá»u 2 luáº­t 59/2020/QH14 doanh nghiá»‡p</td>\n",
       "      <td>Äá»‘i tÆ°á»£ng Ã¡p dá»¥ng 1. Doanh nghiá»‡p. 2. CÆ¡ quan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Äiá»u 3</td>\n",
       "      <td>Äiá»u 3 luáº­t 59/2020/QH14 doanh nghiá»‡p</td>\n",
       "      <td>Ãp dá»¥ng Luáº­t Doanh nghiá»‡p vÃ  luáº­t khÃ¡c TrÆ°á»ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Äiá»u 4</td>\n",
       "      <td>Äiá»u 4 luáº­t 59/2020/QH14 doanh nghiá»‡p</td>\n",
       "      <td>Giáº£i thÃ­ch tá»« ngá»¯ Trong Luáº­t nÃ y, cÃ¡c tá»« ngá»¯ d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Äiá»u 4</td>\n",
       "      <td>Äiá»u 4 luáº­t 59/2020/QH14 doanh nghiá»‡p</td>\n",
       "      <td>11. Doanh nghiá»‡p nhÃ  nÆ°á»›c bao gá»“m cÃ¡c doanh ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Äiá»u 84</td>\n",
       "      <td>Äiá»u 84 luáº­t 24-L/CTN on environment</td>\n",
       "      <td>NgÆ°á»i nÆ°á»›c ngoÃ i cÃ³ hÃ nh vi vi pháº¡m phÃ¡p luáº­t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Äiá»u 86</td>\n",
       "      <td>Äiá»u 86 luáº­t 24-L/CTN on environment</td>\n",
       "      <td>NgÆ°á»i nÃ o lá»£i dá»¥ng chá»©c vá»¥, quyá»n háº¡n hoáº·c vÆ°á»£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Äiá»u 87</td>\n",
       "      <td>Äiá»u 87 luáº­t 24-L/CTN on environment</td>\n",
       "      <td>NgÆ°á»i nÃ o cÃ³ hÃ nh vi vi pháº¡m phÃ¡p luáº­t Ä‘áº¥t Ä‘ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Äiá»u 88</td>\n",
       "      <td>Äiá»u 88 luáº­t 24-L/CTN on environment</td>\n",
       "      <td>Luáº­t nÃ y thay tháº¿ Luáº­t Ä‘áº¥t Ä‘ai Ä‘Ã£ Ä‘Æ°á»£c Quá»‘c há»™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>24-L/CTN</td>\n",
       "      <td>Äiá»u 89</td>\n",
       "      <td>Äiá»u 89 luáº­t 24-L/CTN on environment</td>\n",
       "      <td>ChÃ­nh phá»§ quy Ä‘á»‹nh chi tiáº¿t thi hÃ nh Luáº­t nÃ y....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9254 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           so_hieu     dieu                                 prefix  \\\n",
       "0     59/2020/QH14   Äiá»u 1  Äiá»u 1 luáº­t 59/2020/QH14 doanh nghiá»‡p   \n",
       "1     59/2020/QH14   Äiá»u 2  Äiá»u 2 luáº­t 59/2020/QH14 doanh nghiá»‡p   \n",
       "2     59/2020/QH14   Äiá»u 3  Äiá»u 3 luáº­t 59/2020/QH14 doanh nghiá»‡p   \n",
       "3     59/2020/QH14   Äiá»u 4  Äiá»u 4 luáº­t 59/2020/QH14 doanh nghiá»‡p   \n",
       "4     59/2020/QH14   Äiá»u 4  Äiá»u 4 luáº­t 59/2020/QH14 doanh nghiá»‡p   \n",
       "...            ...      ...                                    ...   \n",
       "9249      24-L/CTN  Äiá»u 84   Äiá»u 84 luáº­t 24-L/CTN on environment   \n",
       "9250      24-L/CTN  Äiá»u 86   Äiá»u 86 luáº­t 24-L/CTN on environment   \n",
       "9251      24-L/CTN  Äiá»u 87   Äiá»u 87 luáº­t 24-L/CTN on environment   \n",
       "9252      24-L/CTN  Äiá»u 88   Äiá»u 88 luáº­t 24-L/CTN on environment   \n",
       "9253      24-L/CTN  Äiá»u 89   Äiá»u 89 luáº­t 24-L/CTN on environment   \n",
       "\n",
       "                                         truncated_text  \n",
       "0     Pháº¡m vi Ä‘iá»u chá»‰nh Luáº­t nÃ y quy Ä‘á»‹nh vá» viá»‡c t...  \n",
       "1     Äá»‘i tÆ°á»£ng Ã¡p dá»¥ng 1. Doanh nghiá»‡p. 2. CÆ¡ quan,...  \n",
       "2     Ãp dá»¥ng Luáº­t Doanh nghiá»‡p vÃ  luáº­t khÃ¡c TrÆ°á»ng ...  \n",
       "3     Giáº£i thÃ­ch tá»« ngá»¯ Trong Luáº­t nÃ y, cÃ¡c tá»« ngá»¯ d...  \n",
       "4     11. Doanh nghiá»‡p nhÃ  nÆ°á»›c bao gá»“m cÃ¡c doanh ng...  \n",
       "...                                                 ...  \n",
       "9249  NgÆ°á»i nÆ°á»›c ngoÃ i cÃ³ hÃ nh vi vi pháº¡m phÃ¡p luáº­t ...  \n",
       "9250  NgÆ°á»i nÃ o lá»£i dá»¥ng chá»©c vá»¥, quyá»n háº¡n hoáº·c vÆ°á»£...  \n",
       "9251  NgÆ°á»i nÃ o cÃ³ hÃ nh vi vi pháº¡m phÃ¡p luáº­t Ä‘áº¥t Ä‘ai...  \n",
       "9252  Luáº­t nÃ y thay tháº¿ Luáº­t Ä‘áº¥t Ä‘ai Ä‘Ã£ Ä‘Æ°á»£c Quá»‘c há»™...  \n",
       "9253  ChÃ­nh phá»§ quy Ä‘á»‹nh chi tiáº¿t thi hÃ nh Luáº­t nÃ y....  \n",
       "\n",
       "[9254 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Setting Parameters and Loading Data\n",
    "data_file = \"sent_truncated_vbpl_legal_only.csv\"  # Update path if necessary.\n",
    "cache_file = \"embeddings_cache.pkl\"               # For caching embeddings.\n",
    "index_file = \"faiss_index.bin\"                    # For saving/loading the FAISS index.\n",
    "state_file = \"custom_state.pkl\"                   # For saving/loading custom state (DataFrame, embeddings).\n",
    "batch_size = 32\n",
    "use_faiss = True\n",
    "\n",
    "finder = LegalDocumentFinder()\n",
    "\n",
    "# Load and preprocess legal documents.\n",
    "finder.load_data(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91291563-ab15-45da-8fde-5bb40e056ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:No FAISS index available to save.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Compute/Load Embeddings, Build FAISS Index, and Save State\n",
    "if not finder.load_cached_embeddings(cache_file):\n",
    "    texts = finder.df[\"truncated_text\"].tolist()\n",
    "    finder.batch_encode_texts(texts, batch_size=batch_size)\n",
    "    # Save embeddings in DataFrame and cache them.\n",
    "    finder.df[\"embedding\"] = list(finder.embeddings)\n",
    "    finder.cache_embeddings(cache_file)\n",
    "else:\n",
    "    logger.info(\"Using cached embeddings.\")\n",
    "\n",
    "if use_faiss:\n",
    "    finder.build_index()\n",
    "    finder.save_index(index_file)  # Save the FAISS index to disk.\n",
    "\n",
    "# Save the custom state (DataFrame and embeddings) for future use.\n",
    "finder.save_state(state_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29686774-ad39-40b0-967b-487d4c566925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save the Hugging Face Model and Tokenizer\n",
    "model_dir = \"phobert_saved_model\"  # Define directory where to save the model.\n",
    "finder.save_model(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2755049f-4e5a-4de8-a620-ee04f84d0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Top Related Articles:\n",
      "\n",
      "ğŸ“˜ Äiá»u 39 (Similarity: 0.68)\n",
      "ğŸ“ TÃ¹ chung thÃ¢n TÃ¹ chung thÃ¢n lÃ  hÃ¬nh pháº¡t tÃ¹ khÃ´ng thá»i háº¡n Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»‘i vá»›i ngÆ°á»i pháº¡m tá»™i Ä‘áº·c biá»‡t nghiÃªm trá»ng, nhÆ°ng chÆ°a Ä‘áº¿n má»©c bá»‹ xá»­ pháº¡t tá»­ hÃ¬nh. KhÃ´ng Ã¡p dá»¥ng hÃ¬nh pháº¡t tÃ¹ chung thÃ¢n Ä‘á»‘i vá»›i ngÆ°á»i dÆ°á»›i 18 tuá»•i pháº¡m tá»™i.\n",
      "\n",
      "ğŸ“˜ Äiá»u 347 (Similarity: 0.67)\n",
      "ğŸ“ Tá»™i vi pháº¡m quy Ä‘á»‹nh vá» xuáº¥t cáº£nh, nháº­p cáº£nh; tá»™i á»Ÿ láº¡i Viá»‡t Nam trÃ¡i phÃ©p NgÆ°á»i nÃ o xuáº¥t cáº£nh, nháº­p cáº£nh trÃ¡i phÃ©p hoáº·c á»Ÿ láº¡i Viá»‡t Nam trÃ¡i phÃ©p, Ä‘Ã£ bá»‹ xá»­ pháº¡t vi pháº¡m hÃ nh chÃ­nh vá» hÃ nh vi nÃ y mÃ  cÃ²n vi pháº¡m, thÃ¬ bá»‹ pháº¡t tiá»n tá»« 5.000.000 Ä‘á»“ng Ä‘áº¿n 50.000.000 Ä‘á»“ng hoáº·c pháº¡t tÃ¹ tá»« 06 thÃ¡ng Ä‘áº¿n 03 nÄƒm.\n",
      "\n",
      "ğŸ“˜ Äiá»u 130 (Similarity: 0.67)\n",
      "ğŸ“ Tá»™i xÃ¢m pháº¡m quyá»n bÃ¬nh Ä‘áº³ng cá»§a phá»¥ ná»¯ NgÆ°á»i nÃ o dÃ¹ng vÅ© lá»±c hoáº·c cÃ³ hÃ nh vi nghiÃªm trá»ng khÃ¡c cáº£n trá»Ÿ phá»¥ ná»¯ tham gia hoáº¡t Ä‘á»™ng chÃ­nh trá»‹, kinh táº¿, khoa há»c, vÄƒn hoÃ¡, xÃ£ há»™i, thÃ¬ bá»‹ pháº¡t cáº£nh cÃ¡o, cáº£i táº¡o khÃ´ng giam giá»¯ Ä‘áº¿n má»™t nÄƒm hoáº·c pháº¡t tÃ¹ tá»« ba thÃ¡ng Ä‘áº¿n má»™t nÄƒm.\n",
      "\n",
      "ğŸ“˜ Äiá»u 94 (Similarity: 0.66)\n",
      "ğŸ“ Tá»™i giáº¿t con má»›i Ä‘áº» NgÆ°á»i máº¹ nÃ o do áº£nh hÆ°á»Ÿng náº·ng ná» cá»§a tÆ° tÆ°á»Ÿng láº¡c háº­u hoáº·c trong hoÃ n cáº£nh khÃ¡ch quan Ä‘áº·c biá»‡t mÃ  giáº¿t con má»›i Ä‘áº» hoáº·c vá»©t bá» Ä‘á»©a tráº» Ä‘Ã³ dáº«n Ä‘áº¿n háº­u quáº£ Ä‘á»©a tráº» cháº¿t, thÃ¬ bá»‹ pháº¡t cáº£i táº¡o khÃ´ng giam giá»¯ Ä‘áº¿n hai nÄƒm hoáº·c pháº¡t tÃ¹ tá»« ba thÃ¡ng Ä‘áº¿n hai nÄƒm.\n",
      "\n",
      "ğŸ“˜ Äiá»u 152 (Similarity: 0.66)\n",
      "ğŸ“ Tá»™i tá»« chá»‘i hoáº·c trá»‘n trÃ¡nh nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng NgÆ°á»i nÃ o cÃ³ nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng vÃ  cÃ³ kháº£ nÄƒng thá»±c táº¿ Ä‘á»ƒ thá»±c hiá»‡n viá»‡c cáº¥p dÆ°á»¡ng Ä‘á»‘i vá»›i ngÆ°á»i mÃ  mÃ¬nh cÃ³ nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t mÃ  cá»‘ Ã½ tá»« chá»‘i hoáº·c trá»‘n trÃ¡nh nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng gÃ¢y háº­u quáº£ nghiÃªm trá»ng hoáº·c Ä‘Ã£ bá»‹ xá»­ pháº¡t hÃ nh chÃ­nh vá» hÃ nh vi nÃ y mÃ  cÃ²n vi pháº¡m, thÃ¬ bá»‹ pháº¡t cáº£nh cÃ¡o, cáº£i táº¡o khÃ´ng giam giá»¯ Ä‘áº¿n hai nÄƒm hoáº·c pháº¡t tÃ¹ tá»« ba thÃ¡ng Ä‘áº¿n hai nÄƒm.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dieu</th>\n",
       "      <th>truncated_text</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Äiá»u 39</td>\n",
       "      <td>TÃ¹ chung thÃ¢n TÃ¹ chung thÃ¢n lÃ  hÃ¬nh pháº¡t tÃ¹ khÃ´ng thá»i háº¡n Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»‘i vá»›i ngÆ°á»i pháº¡m tá»™i Ä‘áº·c biá»‡t nghiÃªm trá»ng, nhÆ°ng chÆ°a Ä‘áº¿n má»©c bá»‹ xá»­ pháº¡t tá»­ hÃ¬nh. KhÃ´ng Ã¡p dá»¥ng hÃ¬nh pháº¡t tÃ¹ chung thÃ¢n Ä‘á»‘i vá»›i ngÆ°á»i dÆ°á»›i 18 tuá»•i pháº¡m tá»™i.</td>\n",
       "      <td>0.677016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Äiá»u 347</td>\n",
       "      <td>Tá»™i vi pháº¡m quy Ä‘á»‹nh vá» xuáº¥t cáº£nh, nháº­p cáº£nh; tá»™i á»Ÿ láº¡i Viá»‡t Nam trÃ¡i phÃ©p NgÆ°á»i nÃ o xuáº¥t cáº£nh, nháº­p cáº£nh trÃ¡i phÃ©p hoáº·c á»Ÿ láº¡i Viá»‡t Nam trÃ¡i phÃ©p, Ä‘Ã£ bá»‹ xá»­ pháº¡t vi pháº¡m hÃ nh chÃ­nh vá» hÃ nh vi nÃ y mÃ  cÃ²n vi pháº¡m, thÃ¬ bá»‹ pháº¡t tiá»n tá»« 5.000.000 Ä‘á»“ng Ä‘áº¿n 50.000.000 Ä‘á»“ng hoáº·c pháº¡t tÃ¹ tá»« 06 thÃ¡ng Ä‘áº¿n 03 nÄƒm.</td>\n",
       "      <td>0.673796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Äiá»u 130</td>\n",
       "      <td>Tá»™i xÃ¢m pháº¡m quyá»n bÃ¬nh Ä‘áº³ng cá»§a phá»¥ ná»¯ NgÆ°á»i nÃ o dÃ¹ng vÅ© lá»±c hoáº·c cÃ³ hÃ nh vi nghiÃªm trá»ng khÃ¡c cáº£n trá»Ÿ phá»¥ ná»¯ tham gia hoáº¡t Ä‘á»™ng chÃ­nh trá»‹, kinh táº¿, khoa há»c, vÄƒn hoÃ¡, xÃ£ há»™i, thÃ¬ bá»‹ pháº¡t cáº£nh cÃ¡o, cáº£i táº¡o khÃ´ng giam giá»¯ Ä‘áº¿n má»™t nÄƒm hoáº·c pháº¡t tÃ¹ tá»« ba thÃ¡ng Ä‘áº¿n má»™t nÄƒm.</td>\n",
       "      <td>0.672321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Äiá»u 94</td>\n",
       "      <td>Tá»™i giáº¿t con má»›i Ä‘áº» NgÆ°á»i máº¹ nÃ o do áº£nh hÆ°á»Ÿng náº·ng ná» cá»§a tÆ° tÆ°á»Ÿng láº¡c háº­u hoáº·c trong hoÃ n cáº£nh khÃ¡ch quan Ä‘áº·c biá»‡t mÃ  giáº¿t con má»›i Ä‘áº» hoáº·c vá»©t bá» Ä‘á»©a tráº» Ä‘Ã³ dáº«n Ä‘áº¿n háº­u quáº£ Ä‘á»©a tráº» cháº¿t, thÃ¬ bá»‹ pháº¡t cáº£i táº¡o khÃ´ng giam giá»¯ Ä‘áº¿n hai nÄƒm hoáº·c pháº¡t tÃ¹ tá»« ba thÃ¡ng Ä‘áº¿n hai nÄƒm.</td>\n",
       "      <td>0.663199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Äiá»u 152</td>\n",
       "      <td>Tá»™i tá»« chá»‘i hoáº·c trá»‘n trÃ¡nh nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng NgÆ°á»i nÃ o cÃ³ nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng vÃ  cÃ³ kháº£ nÄƒng thá»±c táº¿ Ä‘á»ƒ thá»±c hiá»‡n viá»‡c cáº¥p dÆ°á»¡ng Ä‘á»‘i vá»›i ngÆ°á»i mÃ  mÃ¬nh cÃ³ nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t mÃ  cá»‘ Ã½ tá»« chá»‘i hoáº·c trá»‘n trÃ¡nh nghÄ©a vá»¥ cáº¥p dÆ°á»¡ng gÃ¢y háº­u quáº£ nghiÃªm trá»ng hoáº·c Ä‘Ã£ bá»‹ xá»­ pháº¡t hÃ nh chÃ­nh vá» hÃ nh vi nÃ y mÃ  cÃ²n vi pháº¡m, thÃ¬ bá»‹ pháº¡t cáº£nh cÃ¡o, cáº£i táº¡o khÃ´ng giam giá»¯ Ä‘áº¿n hai nÄƒm hoáº·c pháº¡t tÃ¹ tá»« ba thÃ¡ng Ä‘áº¿n hai nÄƒm.</td>\n",
       "      <td>0.661434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: Demo Search and Display Results\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "demo_text = \"vi pháº¡m phÃ¡p luáº­t bá»‹ pháº¡t tÃ¹  \"\n",
    "results = finder.find_similar_articles(demo_text, top_k=5)\n",
    "\n",
    "print(\"\\nğŸ” Top Related Articles:\")\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"\\nğŸ“˜ {row['dieu']} (Similarity: {row['similarity']:.2f})\")\n",
    "    print(f\"ğŸ“ {row['truncated_text']}\")\n",
    "\n",
    "display(HTML(results.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d7d54f-1b60-4b8c-8154-52e313ca0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name phobert_saved_model. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Load the dataset\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_truncated_vbpl_legal_only.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# âœ… Check required columns\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncated_text\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncated_text\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column in the CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U sentencepiece\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load PhoBERT model\n",
    "model_name = \"phobert_saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def encode_phobert(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# 2. Load legal dataset with proper encoding\n",
    "df = pd.read_csv(\"sent_truncated_vbpl_legal_only.csv\", encoding=\"utf-16\")  # Change to ISO-8859-1 if needed\n",
    "assert 'truncated_text' in df.columns\n",
    "\n",
    "# 3. Compute embeddings for all legal documents\n",
    "print(\"ğŸ”„ Encoding all legal texts...\")\n",
    "corpus = df[\"truncated_text\"].astype(str).tolist()\n",
    "corpus_embeddings = encode_phobert(corpus)\n",
    "\n",
    "# 4. Input text to find similar articles\n",
    "demo_text = \"BiÃªn báº£n há»p Há»™i Ä‘á»“ng thÃ nh viÃªn pháº£i Ä‘Æ°á»£c giá»¯ nguyÃªn vÃ  khÃ´ng Ä‘Æ°á»£c phÃ©p thay Ä‘á»•i\"\n",
    "query_embedding = encode_phobert([demo_text])\n",
    "\n",
    "# 5. Compute cosine similarity\n",
    "similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "top_k = 5\n",
    "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "# 6. Show results\n",
    "print(\"\\nğŸ” Top Related Articles:\")\n",
    "results = []\n",
    "for idx in top_indices:\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"\\nğŸ“˜ Äiá»u: {row['dieu']} (Similarity: {similarities[idx]:.2f})\")\n",
    "    print(f\"ğŸ“ {row['truncated_text']}\")\n",
    "    results.append({\n",
    "        \"Äiá»u\": row[\"dieu\"],\n",
    "        \"VÄƒn báº£n\": row[\"truncated_text\"],\n",
    "        \"Äá»™ tÆ°Æ¡ng Ä‘á»“ng\": round(similarities[idx], 2)\n",
    "    })\n",
    "\n",
    "# 7. Display as HTML table\n",
    "display(HTML(pd.DataFrame(results).to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34cf18-59c2-43ec-a6cb-91976e452cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
